{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lenguaje de Manipulación de Datos\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/Volumes/Data/big-data/hive-3.1.0/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/Volumes/Data/big-data/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Hive Session ID = b352f216-e6ce-464f-8bcf-24a7001b430d\n",
      "\n",
      "Logging initialized using configuration in jar:file:/Volumes/Data/big-data/hive-3.1.0/lib/hive-common-3.1.0.jar!/hive-log4j2.properties Async: true\n",
      "Hive Session ID = 9e4fbe63-2655-4f74-bc55-2958bb93da18\n",
      "Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run bd.py\n",
    "%hive_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Time taken: 0.797 seconds\n",
      "OK\n",
      "Time taken: 0.157 seconds\n",
      "OK\n",
      "Time taken: 0.009 seconds\n",
      "OK\n",
      "Time taken: 0.327 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP DATABASE IF EXISTS DMLdb CASCADE;\n",
    "CREATE DATABASE DMLdb;\n",
    "USE DMLdb;\n",
    "\n",
    "CREATE TABLE persons (\n",
    "    id INT,\n",
    "    firstname VARCHAR(10),\n",
    "    surname VARCHAR(10),\n",
    "    birthday TIMESTAMP,\n",
    "    color VARCHAR(9),\n",
    "    quantity INT\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSERT\n",
    "\n",
    "    INSERT INTO TABLE tablename VALUES values_row [, values_row ...]\n",
    "    \n",
    "    values_row: \n",
    "       (value [, value ...])\n",
    "       \n",
    "Note que a diferencia de SQL, aca no es posible indicar para que columnas se van a insertar los valores, de tal manera que siempre se deben dar valores para todas las columnas.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID = jdvelasq_20181005133909_5bb203ac-93cb-479e-becf-cefa2fc40759\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Job running in-process (local Hadoop)\n",
      "2018-10-05 13:39:14,012 Stage-1 map = 100%,  reduce = 100%\n",
      "Ended Job = job_local1350480857_0001\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory file:/Volumes/Data/big-data/hive-3.1.0/warehouse/dmldb.db/persons/.hive-staging_hive_2018-10-05_13-39-09_898_5625808311638892639-1/-ext-10000\n",
      "Loading data to table dmldb.persons\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 0 msec\n",
      "OK\n",
      "Time taken: 4.523 seconds\n",
      "OK\n",
      "1\tVivian\tHamilton\t1971-07-08 00:00:00\tgreen\t1\n",
      "Time taken: 0.135 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "--\n",
    "-- Inserta el registro en la tabla.\n",
    "-- Los valores están en el mismo orden de los campos.\n",
    "--\n",
    "INSERT INTO persons VALUES\n",
    "   (1,\"Vivian\",\"Hamilton\",\"1971-07-08\",\"green\",1);\n",
    "    \n",
    "SELECT * FROM persons;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID = jdvelasq_20181005133915_39b31c11-b959-469b-932b-d1f727fd66f0\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Job running in-process (local Hadoop)\n",
      "2018-10-05 13:39:16,928 Stage-1 map = 100%,  reduce = 100%\n",
      "Ended Job = job_local1759984619_0002\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory file:/Volumes/Data/big-data/hive-3.1.0/warehouse/dmldb.db/persons/.hive-staging_hive_2018-10-05_13-39-15_097_2789178718226633457-1/-ext-10000\n",
      "Loading data to table dmldb.persons\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 0 msec\n",
      "OK\n",
      "Time taken: 2.114 seconds\n",
      "OK\n",
      "1\tVivian\tHamilton\t1971-07-08 00:00:00\tgreen\t1\n",
      "2\tKaren\tHolcomb\t1974-05-23 00:00:00\tgreen\t4\n",
      "3\tCody\tGarrett\t1973-04-22 00:00:00\torange\t1\n",
      "Time taken: 0.078 seconds, Fetched: 3 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "--\n",
    "-- Inserta varios registros a la vez.\n",
    "-- Los valores deben estar en el mismo orden de los campos.\n",
    "--\n",
    "INSERT INTO persons VALUES\n",
    "    (2,\"Karen\",\"Holcomb\",\"1974-05-23\",\"green\",4),\n",
    "    (3,\"Cody\",\"Garrett\",\"1973-04-22\",\"orange\",1);\n",
    "    \n",
    "SELECT * FROM persons;    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPDATE\n",
    "\n",
    "    UPDATE tablename SET column = value [, column = value ...] [WHERE expression]\n",
    "\n",
    "Véase https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DELETE\n",
    "\n",
    "    DELETE FROM tablename [WHERE expression]\n",
    "    \n",
    "Véase https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGE\n",
    "\n",
    "    MERGE INTO <target table> AS T USING <source expression/table> AS S\n",
    "    ON <boolean expression1>\n",
    "    WHEN MATCHED [AND <boolean expression2>] THEN UPDATE SET <set clause list>\n",
    "    WHEN MATCHED [AND <boolean expression3>] THEN DELETE\n",
    "    WHEN NOT MATCHED [AND <boolean expression4>] THEN INSERT VALUES<value list>\n",
    "    \n",
    "Véase https://community.hortonworks.com/articles/97113/hive-acid-merge-by-example.html    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESCRITURA DE DATOS AL DISCO\n",
    "\n",
    "    INSERT OVERWRITE [LOCAL] DIRECTORY directory1\n",
    "      [ROW FORMAT row_format] [STORED AS file_format] \n",
    "      SELECT ... FROM ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el archivo requerido ya tiene el formato deseado, es posible copiarlo directamente del sistema de archivos. En caso contrario, puede usar un INSERT para realizar la conversión de formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## crea el directorio persons-data\n",
    "!rm -rf DMLexport-dir\n",
    "!mkdir DMLexport-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID = jdvelasq_20181005133917_7171dcc6-8c6e-40e3-91d5-104af27d425d\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Job running in-process (local Hadoop)\n",
      "2018-10-05 13:39:19,525 Stage-1 map = 100%,  reduce = 0%\n",
      "Ended Job = job_local272107340_0003\n",
      "Moving data to local directory DMLexport-dir\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 0 msec\n",
      "OK\n",
      "Time taken: 1.639 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "INSERT OVERWRITE LOCAL DIRECTORY 'DMLexport-dir'\n",
    "ROW FORMAT \n",
    "DELIMITED FIELDS TERMINATED BY ','\n",
    "SELECT * FROM persons;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMLexport-dir/000000_0\n"
     ]
    }
   ],
   "source": [
    "!ls DMLexport-dir/* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,Vivian,Hamilton,1971-07-08 00:00:00,green,1\n",
      "2,Karen,Holcomb,1974-05-23 00:00:00,green,4\n",
      "3,Cody,Garrett,1973-04-22 00:00:00,orange,1\n"
     ]
    }
   ],
   "source": [
    "!cat DMLexport-dir/* > DMLexport-data.csv\n",
    "!cat DMLexport-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se borra el archivo si existe\n",
    "!rm -rf DMLexport-dir  DMLexport-data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing persons.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile persons.csv\n",
    "4,Roth,Fry,1975-01-29 00:00:00,black,1\n",
    "5,Zoe,Conway,1974-07-03 00:00:00,blue,2\n",
    "6,Gretchen,Kinney,1974-10-18 00:00:00,viole,1\n",
    "7,Driscoll,Klein,1970-10-05 00:00:00,blue,5\n",
    "8,Karyn,Diaz,1969-02-24 00:00:00,red,1\n",
    "9,Merritt,Guy,1974-10-17 00:00:00,indigo,4\n",
    "10,Kylan,Sexton,1975-02-29 00:00:00,black,4\n",
    "11,Jordan,Estes,1969-12-07 00:00:00,indigo,4\n",
    "12,Hope,Coffey,1973-12-24 00:00:00,green,5\n",
    "13,Vivian,Crane,1970-08-27 00:00:00,gray,5\n",
    "14,Clio,Noel,1972-12-12 00:00:00,red,5\n",
    "15,Hope,Silva,1970-07-01 00:00:00,blue,5\n",
    "16,Ayanna,Jarvis,1974-02-11 00:00:00,orange,5\n",
    "17,Chanda,Boyer,1973-04-01 00:00:00,green,4\n",
    "18,Chadwick,Knight,1973-04-29 00:00:00,yellow,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## prepeara el archivo CSV\n",
    "## para cargarse en Hive\n",
    "## reemplazando las comas por Ctr-A \n",
    "lines = open('persons.csv', 'r').read()\n",
    "lines = lines.replace(',', chr(1))\n",
    "open('persons', 'w').write(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data to table dmldb.persons\n",
      "OK\n",
      "Time taken: 0.104 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "--\n",
    "-- el separador de campos debe ser ctrl-A\n",
    "-- en Python es `chr(1)`\n",
    "--\n",
    "LOAD DATA LOCAL INPATH 'persons' INTO TABLE persons; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "1\tVivian\tHamilton\t1971-07-08 00:00:00\tgreen\t1\n",
      "2\tKaren\tHolcomb\t1974-05-23 00:00:00\tgreen\t4\n",
      "3\tCody\tGarrett\t1973-04-22 00:00:00\torange\t1\n",
      "4\tRoth\tFry\t1975-01-29 00:00:00\tblack\t1\n",
      "5\tZoe\tConway\t1974-07-03 00:00:00\tblue\t2\n",
      "6\tGretchen\tKinney\t1974-10-18 00:00:00\tviole\t1\n",
      "7\tDriscoll\tKlein\t1970-10-05 00:00:00\tblue\t5\n",
      "8\tKaryn\tDiaz\t1969-02-24 00:00:00\tred\t1\n",
      "9\tMerritt\tGuy\t1974-10-17 00:00:00\tindigo\t4\n",
      "10\tKylan\tSexton\t1975-03-01 00:00:00\tblack\t4\n",
      "11\tJordan\tEstes\t1969-12-07 00:00:00\tindigo\t4\n",
      "12\tHope\tCoffey\t1973-12-24 00:00:00\tgreen\t5\n",
      "13\tVivian\tCrane\t1970-08-27 00:00:00\tgray\t5\n",
      "14\tClio\tNoel\t1972-12-12 00:00:00\tred\t5\n",
      "15\tHope\tSilva\t1970-07-01 00:00:00\tblue\t5\n",
      "16\tAyanna\tJarvis\t1974-02-11 00:00:00\torange\t5\n",
      "17\tChanda\tBoyer\t1973-04-01 00:00:00\tgreen\t4\n",
      "18\tChadwick\tKnight\t1973-04-29 00:00:00\tyellow\t1\n",
      "Time taken: 0.08 seconds, Fetched: 18 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM persons;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA con datos complejos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Hive usa cuatro tipos de datos complejos. A coninuación se presentan los separadores por defecto.\n",
    "\n",
    "* Record: \\n o delimitador de filas\n",
    "* FIELD (campos): Ctr+A o \\001, delimita campos.\n",
    "* ARRAY y STRUCT: Ctrl+B o \\002, delimita elementos \n",
    "* MAP: Ctrl+C o \\003, delimita parejas (key, value). \n",
    "* UNIONTYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Time taken: 0.028 seconds\n",
      "OK\n",
      "Time taken: 0.039 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS complextypes;\n",
    "CREATE TABLE complextypes (\n",
    "    id INT,\n",
    "    arrayField ARRAY<STRING>, \n",
    "    structField STRUCT<c21:INT, c22:INT>,\n",
    "    mapField MAP<STRING, INT>\n",
    ")\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY ','\n",
    "COLLECTION ITEMS TERMINATED BY ':'\n",
    "MAP KEYS TERMINATED BY '#'\n",
    "LINES TERMINATED BY '\\n';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing complextypes.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile complextypes.csv\n",
    "1,A:B:C,1:2,key1#1:key2#2\n",
    "2,E:F,3:4,key1#3:key3#4\n",
    "3,G:H:I,5:6,key2#5:key3#6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data to table dmldb.complextypes\n",
      "OK\n",
      "Time taken: 0.079 seconds\n",
      "OK\n",
      "1\t[\"A\",\"B\",\"C\"]\t{\"c21\":1,\"c22\":2}\t{\"key1\":1,\"key2\":2}\n",
      "2\t[\"E\",\"F\"]\t{\"c21\":3,\"c22\":4}\t{\"key1\":3,\"key3\":4}\n",
      "3\t[\"G\",\"H\"]\t{\"c21\":5,\"c22\":6}\t{\"key2\":5,\"key3\":6}\n",
      "Time taken: 0.095 seconds, Fetched: 3 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "LOAD DATA LOCAL INPATH 'complextypes.csv' INTO TABLE complextypes;\n",
    "SELECT * FROM complextypes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "[\"A\",\"B\",\"C\"]\n",
      "[\"E\",\"F\"]\n",
      "[\"G\",\"H\"]\n",
      "Time taken: 0.076 seconds, Fetched: 3 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "--\n",
    "-- Selecciona el campo ARRAY\n",
    "--\n",
    "SELECT arrayField FROM complextypes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "A\n",
      "E\n",
      "G\n",
      "Time taken: 0.1 seconds, Fetched: 3 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT arrayField[0] FROM complextypes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "C\n",
      "NULL\n",
      "NULL\n",
      "Time taken: 0.067 seconds, Fetched: 3 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT arrayField[2] FROM complextypes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "{\"c21\":1,\"c22\":2}\n",
      "{\"c21\":3,\"c22\":4}\n",
      "{\"c21\":5,\"c22\":6}\n",
      "Time taken: 0.068 seconds, Fetched: 3 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT structField FROM complextypes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "1\t2\n",
      "3\t4\n",
      "5\t6\n",
      "Time taken: 0.066 seconds, Fetched: 3 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT structField.c21, structField.c22 FROM complextypes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "{\"key1\":1,\"key2\":2}\n",
      "{\"key1\":3,\"key3\":4}\n",
      "{\"key2\":5,\"key3\":6}\n",
      "Time taken: 0.067 seconds, Fetched: 3 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT mapField FROM complextypes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "1\n",
      "3\n",
      "NULL\n",
      "Time taken: 0.077 seconds, Fetched: 3 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT mapField['key1'] FROM complextypes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "NULL\n",
      "4\n",
      "6\n",
      "Time taken: 0.072 seconds, Fetched: 3 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT mapField['key3'] FROM complextypes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm complextypes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSERT ... SELECT ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible insertar datos desde queries. La sintaxis sería la siguiente:\n",
    "\n",
    "    INSERT OVERWRITE TABLE bancos1\n",
    "    SELECT * FROM Bancos\n",
    "    WHERE .... ;\n",
    "    \n",
    "\n",
    "La clausula anterior es posible reescribirla como:\n",
    "\n",
    "    FROM Bancos se \n",
    "    INSERT OVERWRITE TABLE bancos1\n",
    "       SELECT * WHERE ....\n",
    "    INSERT OVERWRITE TABLE bancos2\n",
    "       SELECT * WHERE ... ;\n",
    "       \n",
    "generando múltiples INSERT a tablas diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Time taken: 0.035 seconds\n",
      "Query ID = jdvelasq_20181005133923_26ce9499-0b5b-4e01-b48a-ec8850e967cc\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Job running in-process (local Hadoop)\n",
      "2018-10-05 13:39:25,210 Stage-1 map = 100%,  reduce = 100%\n",
      "Ended Job = job_local1852017709_0004\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory file:/Volumes/Data/big-data/hive-3.1.0/warehouse/dmldb.db/persons1/.hive-staging_hive_2018-10-05_13-39-23_478_3060559575014796243-1/-ext-10000\n",
      "Loading data to table dmldb.persons1\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 0 msec\n",
      "OK\n",
      "Time taken: 1.975 seconds\n",
      "OK\n",
      "1\tVivian\tHamilton\t1971-07-08 00:00:00\tgreen\t1\n",
      "2\tKaren\tHolcomb\t1974-05-23 00:00:00\tgreen\t4\n",
      "12\tHope\tCoffey\t1973-12-24 00:00:00\tgreen\t5\n",
      "17\tChanda\tBoyer\t1973-04-01 00:00:00\tgreen\t4\n",
      "Time taken: 0.076 seconds, Fetched: 4 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "CREATE TABLE IF NOT EXISTS persons1 LIKE persons;\n",
    "\n",
    "INSERT OVERWRITE TABLE persons1\n",
    "SELECT * FROM persons\n",
    "WHERE color='green';\n",
    "\n",
    "SELECT * FROM persons1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Time taken: 0.031 seconds\n",
      "Query ID = jdvelasq_20181005133926_2f50e9db-de27-4cf4-b823-74ec9381bbdf\n",
      "Total jobs = 4\n",
      "Launching Job 1 out of 4\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Job running in-process (local Hadoop)\n",
      "2018-10-05 13:39:27,863 Stage-2 map = 100%,  reduce = 100%\n",
      "Ended Job = job_local1186420580_0005\n",
      "Stage-5 is selected by condition resolver.\n",
      "Stage-4 is filtered out by condition resolver.\n",
      "Stage-6 is filtered out by condition resolver.\n",
      "Loading data to table dmldb.persons2\n",
      "Launching Job 4 out of 4\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Job running in-process (local Hadoop)\n",
      "2018-10-05 13:39:29,516 Stage-10 map = 100%,  reduce = 100%\n",
      "Ended Job = job_local1107832432_0006\n",
      "Moving data to directory file:/Volumes/Data/big-data/hive-3.1.0/warehouse/dmldb.db/persons1/.hive-staging_hive_2018-10-05_13-39-26_080_8654722329993184881-1/-ext-10000\n",
      "Loading data to table dmldb.persons1\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-2:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Stage-Stage-10:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 0 msec\n",
      "OK\n",
      "Time taken: 3.604 seconds\n",
      "OK\n",
      "8\tKaryn\tDiaz\t1969-02-24 00:00:00\tred\t1\n",
      "14\tClio\tNoel\t1972-12-12 00:00:00\tred\t5\n",
      "Time taken: 0.066 seconds, Fetched: 2 row(s)\n",
      "OK\n",
      "1\tVivian\tHamilton\t1971-07-08 00:00:00\tgreen\t1\n",
      "2\tKaren\tHolcomb\t1974-05-23 00:00:00\tgreen\t4\n",
      "12\tHope\tCoffey\t1973-12-24 00:00:00\tgreen\t5\n",
      "17\tChanda\tBoyer\t1973-04-01 00:00:00\tgreen\t4\n",
      "Time taken: 0.062 seconds, Fetched: 4 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "CREATE TABLE IF NOT EXISTS persons2 LIKE persons;\n",
    "\n",
    "FROM persons\n",
    "INSERT OVERWRITE TABLE persons1\n",
    "   SELECT * WHERE color='red'\n",
    "INSERT OVERWRITE TABLE persons2\n",
    "   SELECT * WHERE color='green';\n",
    "\n",
    "SELECT * FROM persons1;\n",
    "\n",
    "SELECT * FROM persons2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTITIONED BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Time taken: 0.004 seconds\n",
      "OK\n",
      "Time taken: 0.16 seconds\n",
      "Query ID = jdvelasq_20181005133930_bf5a49f8-39e4-4783-94b4-273b1a911708\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Job running in-process (local Hadoop)\n",
      "2018-10-05 13:39:32,493 Stage-1 map = 100%,  reduce = 100%\n",
      "Ended Job = job_local287750212_0007\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory file:/Volumes/Data/big-data/hive-3.1.0/warehouse/dmldb.db/persons3/.hive-staging_hive_2018-10-05_13-39-30_902_7374912572334412211-1/-ext-10000\n",
      "Loading data to table dmldb.persons3 partition (quantity=null)\n",
      "\t Time taken to load dynamic partitions: 0.15 seconds\n",
      "\t Time taken for adding to write entity : 0.001 seconds\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 0 msec\n",
      "OK\n",
      "Time taken: 2.202 seconds\n",
      "OK\n",
      "4\tRoth\tFry\t1975-01-29 00:00:00\tblack\t1\n",
      "6\tGretchen\tKinney\t1974-10-18 00:00:00\tviole\t1\n",
      "8\tKaryn\tDiaz\t1969-02-24 00:00:00\tred\t1\n",
      "18\tChadwick\tKnight\t1973-04-29 00:00:00\tyellow\t1\n",
      "5\tZoe\tConway\t1974-07-03 00:00:00\tblue\t2\n",
      "9\tMerritt\tGuy\t1974-10-17 00:00:00\tindigo\t4\n",
      "10\tKylan\tSexton\t1975-03-01 00:00:00\tblack\t4\n",
      "11\tJordan\tEstes\t1969-12-07 00:00:00\tindigo\t4\n",
      "17\tChanda\tBoyer\t1973-04-01 00:00:00\tgreen\t4\n",
      "7\tDriscoll\tKlein\t1970-10-05 00:00:00\tblue\t5\n",
      "12\tHope\tCoffey\t1973-12-24 00:00:00\tgreen\t5\n",
      "13\tVivian\tCrane\t1970-08-27 00:00:00\tgray\t5\n",
      "14\tClio\tNoel\t1972-12-12 00:00:00\tred\t5\n",
      "15\tHope\tSilva\t1970-07-01 00:00:00\tblue\t5\n",
      "16\tAyanna\tJarvis\t1974-02-11 00:00:00\torange\t5\n",
      "Time taken: 0.103 seconds, Fetched: 15 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS persons3;\n",
    "\n",
    "CREATE TABLE persons3 (\n",
    "    id INT,\n",
    "    firstname VARCHAR(10),\n",
    "    surname VARCHAR(10),\n",
    "    birthday TIMESTAMP,\n",
    "    color VARCHAR(9)\n",
    ")\n",
    "PARTITIONED BY (quantity INT);\n",
    "\n",
    "LOAD DATA LOCAL INPATH 'persons' INTO TABLE persons3; \n",
    "\n",
    "SELECT * FROM persons3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxr-xr-x  4 jdvelasq  staff  128 Oct  5 13:39 \u001b[36mquantity=1\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 jdvelasq  staff  128 Oct  5 13:39 \u001b[36mquantity=2\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 jdvelasq  staff  128 Oct  5 13:39 \u001b[36mquantity=4\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 jdvelasq  staff  128 Oct  5 13:39 \u001b[36mquantity=5\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -l /Volumes/Data/big-data/hive-3.1.0/warehouse/dmldb.db/persons3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "-rw-r--r--  1 jdvelasq  staff  164 Oct  5 13:39 000000_0\n"
     ]
    }
   ],
   "source": [
    "!ls -l /Volumes/Data/big-data/hive-3.1.0/warehouse/dmldb.db/persons3/quantity=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\u0001Roth\u0001Fry\u00011975-01-29 00:00:00\u0001black\n",
      "6\u0001Gretchen\u0001Kinney\u00011974-10-18 00:00:00\u0001viole\n",
      "8\u0001Karyn\u0001Diaz\u00011969-02-24 00:00:00\u0001red\n",
      "18\u0001Chadwick\u0001Knight\u00011973-04-29 00:00:00\u0001yellow\n"
     ]
    }
   ],
   "source": [
    "!cat /Volumes/Data/big-data/hive-3.1.0/warehouse/dmldb.db/persons3/quantity=1/000000_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\u0001Zoe\u0001Conway\u00011974-07-03 00:00:00\u0001blue\n"
     ]
    }
   ],
   "source": [
    "!cat /Volumes/Data/big-data/hive-3.1.0/warehouse/dmldb.db/persons3/quantity=2/000000_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\u0001Merritt\u0001Guy\u00011974-10-17 00:00:00\u0001indigo\n",
      "10\u0001Kylan\u0001Sexton\u00011975-03-01 00:00:00\u0001black\n",
      "11\u0001Jordan\u0001Estes\u00011969-12-07 00:00:00\u0001indigo\n",
      "17\u0001Chanda\u0001Boyer\u00011973-04-01 00:00:00\u0001green\n"
     ]
    }
   ],
   "source": [
    "!cat /Volumes/Data/big-data/hive-3.1.0/warehouse/dmldb.db/persons3/quantity=4/000000_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\u0001Driscoll\u0001Klein\u00011970-10-05 00:00:00\u0001blue\n",
      "12\u0001Hope\u0001Coffey\u00011973-12-24 00:00:00\u0001green\n",
      "13\u0001Vivian\u0001Crane\u00011970-08-27 00:00:00\u0001gray\n",
      "14\u0001Clio\u0001Noel\u00011972-12-12 00:00:00\u0001red\n",
      "15\u0001Hope\u0001Silva\u00011970-07-01 00:00:00\u0001blue\n",
      "16\u0001Ayanna\u0001Jarvis\u00011974-02-11 00:00:00\u0001orange\n"
     ]
    }
   ],
   "source": [
    "!cat /Volumes/Data/big-data/hive-3.1.0/warehouse/dmldb.db/persons3/quantity=5/000000_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Time taken: 0.655 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "-- limpia la base de datos\n",
    "DROP DATABASE IF EXISTS DMLdb CASCADE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm persons*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
