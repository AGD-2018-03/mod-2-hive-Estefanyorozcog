{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducción\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución de Hive en la consola de comandos\n",
    "\n",
    "Esta es la forma más común de trabajo. La bodega de datos debe ser creada en la carpeta donde se invocará `hive` con el siguiente comando:\n",
    "\n",
    "     $HIVE_HOME/bin/schematool -dbType derby -initSchema\n",
    "     \n",
    "Note que si invoca `hive` desde otra carpeta se generará un error. En otras palabras, use el comando anterior en la carpeta donde descargo esta serie de tutoriales para que los comandos puedan ejecutarse desde Jupyter.\n",
    "\n",
    "Adicionalmente debe hacer la capeta `/tmp/hive` escribible con `sudo chmod 777 /tmp/hive`.\n",
    "\n",
    "La primera vez que se ejecuta Hive, crea en la carpeta actual, las carpetas:\n",
    "\n",
    "* metastore_db: contiene los metadatos\n",
    "\n",
    "\n",
    "* warehouse: contiene las bases de datos y las tablas\n",
    "\n",
    "También es posible enviar comandos y salir inmediatamente de `hive`. Para ello, use la opción `-e`; por ejemplo, en Terminal digite:\n",
    "\n",
    "    hive -e \"SHOW DATABASES;\"\n",
    "    \n",
    "La opción `-S` corresponde al modo silencioso, en el que se suprime información adicional. En conjunción con el operador `>` hace posible la generación de archivos de texto con los resultados de la ejecución de uno o más comandos:\n",
    "\n",
    "    hive -S -e \"SELECT * FROM table LIMIT 3;\" > result.txt\n",
    "    \n",
    "    \n",
    "Adicionalmente, es posible almacenar secuencias de comandos en archivos (usualmente con extensión \".q\" o \".hql\" que pueden ejecutarse desde `hive`. En el prompt de `hive` use el comando `source`:\n",
    "\n",
    "    hive> source miprog.hql\n",
    "    \n",
    "    \n",
    "y en la línea de comandos\n",
    "\n",
    "    hive -S -e \"source miprog.hql\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución de comandos del sistema operativo desde Hive\n",
    "\n",
    "`Hive` permite la ejecución de comandos del sistema operativo usando `!`; por ejemplo:\n",
    "\n",
    "     hive> ! ls\n",
    "     \n",
    "También es posible usar comandos del sistema HDFS; el comando `hadoop dfs -ls /` se escribiría en `hive` como\n",
    "\n",
    "     hive> dfs -ls / ;\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentarios\n",
    "\n",
    "Se usan los dos guiones `--`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de Hive en Jupyter\n",
    "\n",
    "A continuación se describe como ejecutar comandos de Hive en Jupyter. El archivo `hivemagic.py` contiene el codigo para ejecutar comandos de Hive directamente en las celdas de un libro de Jupyter. Para instalar el magic, simplemente ejecute el código de dicho archivo.\n",
    "\n",
    "Modifique el archivo `bd.py` indicando el directorio de instalación de Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/Volumes/Data/big-data/hive-3.1.0/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/Volumes/Data/big-data/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Hive Session ID = 7b873911-722f-4f25-860c-21542f35f33f\n",
      "\n",
      "Logging initialized using configuration in jar:file:/Volumes/Data/big-data/hive-3.1.0/lib/hive-common-3.1.0.jar!/hive-log4j2.properties Async: true\n",
      "Hive Session ID = 31f8e8c4-eb69-4e15-bc46-959970960b49\n",
      "Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run bd.py\n",
    "%hive_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Time taken: 1.419 seconds\n",
      "OK\n",
      "Time taken: 0.083 seconds\n",
      "OK\n",
      "Time taken: 0.306 seconds\n",
      "Loading data to table default.docs\n",
      "OK\n",
      "Time taken: 0.237 seconds\n",
      "Query ID = jdvelasq_20181017135508_bddeb366-67cf-4ff3-b5ab-86203aa5eb6f\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Job running in-process (local Hadoop)\n",
      "2018-10-17 13:55:12,090 Stage-1 map = 100%,  reduce = 100%\n",
      "Ended Job = job_local786895292_0001\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Job running in-process (local Hadoop)\n",
      "2018-10-17 13:55:13,622 Stage-2 map = 100%,  reduce = 100%\n",
      "Ended Job = job_local1588878394_0002\n",
      "Moving data to directory file:/Volumes/Data/hive-warehouse/word_counts\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Stage-Stage-2:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 0 msec\n",
      "OK\n",
      "Time taken: 5.303 seconds\n",
      "OK\n",
      "\t20\n",
      "(DA)\t1\n",
      "(see\t1\n",
      "Analytics\t2\n",
      "Analytics,\t1\n",
      "Big\t1\n",
      "Data\t3\n",
      "Especially\t1\n",
      "Organizations\t1\n",
      "Since\t1\n",
      "Time taken: 0.247 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS docs;\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "\n",
    "CREATE TABLE docs (line STRING);\n",
    "\n",
    "LOAD DATA LOCAL INPATH \n",
    "    'files/wordcount' \n",
    "OVERWRITE INTO TABLE docs;\n",
    "\n",
    "CREATE TABLE word_counts \n",
    "AS\n",
    "    SELECT word, count(1) AS count \n",
    "    FROM\n",
    "        (SELECT explode(split(line, '\\\\s')) AS word FROM docs) w\n",
    "GROUP BY \n",
    "    word\n",
    "ORDER BY \n",
    "    word;\n",
    "\n",
    "SELECT * FROM word_counts LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
